{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1D CNN**"
      ],
      "metadata": {
        "id": "4w8rbPrTsYqb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wr4vrn7iYChE",
        "outputId": "2b346e99-6e02-4d51-d691-6315a482841c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "replace afdb_data/files/08455.hea? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "Requirement already satisfied: wfdb in /usr/local/lib/python3.12/dist-packages (4.3.0)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.13.2)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2025.3.0)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.2.3 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.3.3)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (1.16.3)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.22.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->wfdb) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2025.11.12)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp>=3.10.11->wfdb) (4.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "zip_path = '/content/drive/MyDrive/semester_project_2.3/mit-bih-atrial-fibrillation-database-1.0.0.zip'\n",
        "!cp \"{zip_path}\" /content/afdb.zip\n",
        "!mkdir -p afdb_data\n",
        "!unzip -q /content/afdb.zip -d afdb_data\n",
        "!pip install wfdb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wfdb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from scipy.signal import butter, filtfilt\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, BatchNormalization, MaxPooling1D, SpatialDropout1D, GlobalAveragePooling1D, Dropout, Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "2wvNt0ezfxkJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find usable patients\n",
        "Not all of our patients are usable for classification. Some are missing their\n",
        ".dat files and others include AFL or J labels\n",
        "\n",
        "Let's clear them up"
      ],
      "metadata": {
        "id": "PcZjNVeFtTbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# locate all patients with ecg signals (.dat files)\n",
        "data_dir = \"/content/afdb_data/files\"\n",
        "all_files = os.listdir(data_dir)\n",
        "dat_files = [f for f in all_files if f.endswith('.dat')]\n",
        "\n",
        "# patients with .dat files\n",
        "dat_patients = sorted([f.replace('.dat', '') for f in dat_files])\n",
        "\n",
        "# all unique patient IDs in the folder\n",
        "all_patients = sorted(set(f.split('.')[0] for f in all_files))\n",
        "# exclude patients with no .dat file\n",
        "excluded_patients = sorted([p for p in all_patients if p not in dat_patients])\n",
        "# remove non-patient entries\n",
        "non_patient_entries = ['ANNOTATORS', 'RECORDS', 'SHA256SUMS', 'notes', 'old']\n",
        "excluded_patients = [p for p in excluded_patients if p not in non_patient_entries]\n",
        "\n",
        "# print results\n",
        "print(f\"Patients with .dat file: {len(dat_patients)}\")\n",
        "print(dat_patients)\n",
        "print(f\"\\nPatients without .dat file: {len(excluded_patients)}\")\n",
        "print(excluded_patients)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO9V_zsoYhoC",
        "outputId": "8b823cc8-6e3c-494d-c137-716ffb1abfa6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patients with .dat file: 23\n",
            "['04015', '04043', '04048', '04126', '04746', '04908', '04936', '05091', '05121', '05261', '06426', '06453', '06995', '07162', '07859', '07879', '07910', '08215', '08219', '08378', '08405', '08434', '08455']\n",
            "\n",
            "Patients without .dat file: 2\n",
            "['00735', '03665']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make a list of patients strictly with AFib/N signals\n",
        "included_patients = []\n",
        "\n",
        "for patient in dat_patients:\n",
        "    record_path = os.path.join(data_dir, patient)\n",
        "    try:\n",
        "        ann = wfdb.rdann(record_path, 'atr')\n",
        "        aux_notes = set(ann.aux_note)\n",
        "\n",
        "        if aux_notes.issubset({'(AFIB', '(N'}):\n",
        "            included_patients.append(patient)\n",
        "        else:\n",
        "            excluded_patients.append(patient)\n",
        "\n",
        "    except Exception as e:\n",
        "        excluded_patients.append(patient)\n",
        "\n",
        "# upon inspection found very noisy signal\n",
        "included_patients.remove('07859')\n",
        "excluded_patients.append('07859')\n",
        "included_patients.remove('08405')\n",
        "excluded_patients.append('08405')\n",
        "included_patients.remove('08434')\n",
        "excluded_patients.append('08434')\n",
        "\n",
        "# print results\n",
        "print(f\"Included patients (AFIB/N only): {len(included_patients)}\")\n",
        "print(included_patients)\n",
        "print(f\"\\nExcluded patients: {len(excluded_patients)}\")\n",
        "print(excluded_patients)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al8s50JDbcta",
        "outputId": "c89aeafb-3046-4920-a80d-4b7ff4e3778e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Included patients (AFIB/N only): 10\n",
            "['04015', '04048', '04126', '04746', '05091', '05261', '06453', '07162', '08219', '08455']\n",
            "\n",
            "Excluded patients: 15\n",
            "['00735', '03665', '04043', '04908', '04936', '05121', '06426', '06995', '07879', '07910', '08215', '08378', '07859', '08405', '08434']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We have a list of all usable patients in \"included_patients\". Let's check if they compose a balanced dataset"
      ],
      "metadata": {
        "id": "hyiV_toTttPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# count AFib vs N samples across all included patients\n",
        "afib_count = 0\n",
        "normal_count = 0\n",
        "\n",
        "for patient in included_patients:\n",
        "    record_path = os.path.join(data_dir, patient)\n",
        "    record = wfdb.rdrecord(record_path)\n",
        "    signal = record.p_signal[:, 0]\n",
        "    ann = wfdb.rdann(record_path, 'atr')\n",
        "\n",
        "    # create a mask of the same length as signal\n",
        "    mask = np.zeros(len(signal), dtype=int)  # 0 = N, 1 = AFib\n",
        "\n",
        "    current_label = 0\n",
        "    for i in range(len(ann.sample) - 1):\n",
        "        note = ann.aux_note[i]\n",
        "        if note == '(AFIB':\n",
        "            current_label = 1\n",
        "        elif note == '(N':\n",
        "            current_label = 0\n",
        "        mask[ann.sample[i]:ann.sample[i+1]] = current_label\n",
        "    # fill the remainder\n",
        "    mask[ann.sample[-1]:] = current_label\n",
        "\n",
        "    afib_count += np.sum(mask == 1)\n",
        "    normal_count += np.sum(mask == 0)\n",
        "\n",
        "# print results\n",
        "total = afib_count + normal_count\n",
        "print(f\"AFib samples: {afib_count} ({afib_count/total*100:.2f}%)\")\n",
        "print(f\"N samples: {normal_count} ({normal_count/total*100:.2f}%)\")\n",
        "print(f\"Total annotated samples: {total}\")"
      ],
      "metadata": {
        "id": "ik003uliiyYX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6246818a-696a-4eb4-baed-bd1b8349ad2e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AFib samples: 30087575 (33.00%)\n",
            "N samples: 61089265 (67.00%)\n",
            "Total annotated samples: 91176840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Dataset is imbalanced, we'll need to account for this later"
      ],
      "metadata": {
        "id": "JZYAMDjQt_mN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting patients into train/val/test sets\n",
        "We create a function to label windows of desired size, using a predetermined frequency"
      ],
      "metadata": {
        "id": "oAnAo9cNuGQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------\n",
        "# function that extracts labeled windows from a list of patients\n",
        "# --------------------------------------------------------------\n",
        "\n",
        "def extract_labeled_windows(patients, data_dir, window_sec, fs=250):\n",
        "    window_samples = window_sec * fs\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for patient in patients:\n",
        "        try:\n",
        "            record_path = os.path.join(data_dir, patient)\n",
        "            record = wfdb.rdrecord(record_path)\n",
        "            signal = record.p_signal[:, 0] # use first channel\n",
        "            ann = wfdb.rdann(record_path, 'atr')\n",
        "\n",
        "            # create mask of the same length as the signal\n",
        "            mask = np.zeros(len(signal), dtype=int) # 0 = N, 1 = AFib\n",
        "            current_label = 0\n",
        "            for i in range(len(ann.sample) - 1):\n",
        "                note = ann.aux_note[i]\n",
        "                if note == '(AFIB':\n",
        "                    current_label = 1\n",
        "                elif note == '(N':\n",
        "                    current_label = 0\n",
        "                mask[ann.sample[i]:ann.sample[i+1]] = current_label\n",
        "            mask[ann.sample[-1]:] = current_label\n",
        "\n",
        "            # slide windows across signal\n",
        "            step = window_samples  # no overlap\n",
        "            for start in range(0, len(signal) - window_samples + 1, step):\n",
        "                window_signal = signal[start:start+window_samples]\n",
        "                window_mask = mask[start:start+window_samples]\n",
        "\n",
        "                # label window if majority AFib or N\n",
        "                afib_ratio = np.sum(window_mask == 1) / window_samples\n",
        "                normal_ratio = np.sum(window_mask == 0) / window_samples\n",
        "\n",
        "                # only keep windows with 90% majority\n",
        "                if afib_ratio > 0.9:\n",
        "                    X.append(window_signal)\n",
        "                    y.append(1)\n",
        "                elif normal_ratio > 0.9:\n",
        "                    X.append(window_signal)\n",
        "                    y.append(0)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing patient {patient}: {e}\")\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "La2m_CBslYC0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split into train/val/test"
      ],
      "metadata": {
        "id": "ZO5f-vUsuxa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------\n",
        "# define patient splits\n",
        "# ---------------------\n",
        "included_patients_sorted = sorted(included_patients)\n",
        "\n",
        "# 60-20-20 offers best balance between val/test\n",
        "n_patients = len(included_patients_sorted)\n",
        "n_train = int(0.6 * n_patients)\n",
        "n_val   = int(0.2 * n_patients)\n",
        "\n",
        "train_patients = included_patients_sorted[:n_train]\n",
        "val_patients   = included_patients_sorted[n_train:n_train+n_val]\n",
        "test_patients  = included_patients_sorted[n_train+n_val:]\n",
        "\n",
        "# -----------------\n",
        "# generate datasets\n",
        "# -----------------\n",
        "X_train, y_train = extract_labeled_windows(train_patients, data_dir, 10)\n",
        "X_val, y_val     = extract_labeled_windows(val_patients, data_dir, 10)\n",
        "X_test, y_test   = extract_labeled_windows(test_patients, data_dir, 10)\n",
        "\n",
        "# print results\n",
        "print(\"Shapes:\")\n",
        "print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
        "print(\"X_val:  \", X_val.shape, \"y_val:  \", y_val.shape)\n",
        "print(\"X_test: \", X_test.shape, \"y_test: \", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwWCPwNzlXnu",
        "outputId": "87a0fa2f-d36c-4426-fa5a-a56129c05f14"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "X_train: (22027, 2500) y_train: (22027,)\n",
            "X_val:   (7003, 2500) y_val:   (7003,)\n",
            "X_test:  (7297, 2500) y_test:  (7297,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Band Pass Filter"
      ],
      "metadata": {
        "id": "6f7DSAsX028f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def band_pass_filter(signal, fs, lowcut=0.1, highcut=40.0, order=2):\n",
        "    nyquist = 0.5 * fs\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    filtered = filtfilt(b, a, signal)\n",
        "\n",
        "    return filtered"
      ],
      "metadata": {
        "id": "yY8RJZbOsXiO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization"
      ],
      "metadata": {
        "id": "DVXI4WXh0_xZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_signal(signal):\n",
        "    mu = np.mean(signal)\n",
        "    std = np.std(signal)\n",
        "    if std < 1e-8:\n",
        "        return signal\n",
        "    return (signal - mu) / std"
      ],
      "metadata": {
        "id": "4syPKd4M1Bct"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Apply preprocessing"
      ],
      "metadata": {
        "id": "cLQXKygx2YbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------\n",
        "# function that applies BPF and normalization functions to all windows\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "def preprocess_windows(X, fs):\n",
        "    X_preprocessed = np.zeros_like(X, dtype=np.float32)\n",
        "\n",
        "    for i in tqdm(range(len(X)), desc=\"Preprocessing splits\"):\n",
        "        window = X[i]\n",
        "        filtered = band_pass_filter(window, fs)\n",
        "        normalized = normalize_signal(filtered)\n",
        "        X_preprocessed[i] = normalized\n",
        "\n",
        "    return X_preprocessed"
      ],
      "metadata": {
        "id": "YwmzE3tN1s86"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply preprocessing to all splits\n",
        "X_train = preprocess_windows(X_train, 250)\n",
        "X_val   = preprocess_windows(X_val, 250)\n",
        "X_test  = preprocess_windows(X_test, 250)\n",
        "\n",
        "# print results\n",
        "print(\"\\n\\nShapes after preprocessing:\")\n",
        "print(\"X_train:\", X_train.shape)\n",
        "print(\"X_val:  \", X_val.shape)\n",
        "print(\"X_test: \", X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "517LAB9C2Skn",
        "outputId": "ee2f3c37-f0e4-473e-f7ab-b57d264619d6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocessing splits: 100%|██████████| 22027/22027 [00:22<00:00, 999.73it/s] \n",
            "Preprocessing splits: 100%|██████████| 7003/7003 [00:06<00:00, 1093.84it/s]\n",
            "Preprocessing splits: 100%|██████████| 7297/7297 [00:07<00:00, 1005.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Shapes after preprocessing:\n",
            "X_train: (22027, 2500)\n",
            "X_val:   (7003, 2500)\n",
            "X_test:  (7297, 2500)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train CNN"
      ],
      "metadata": {
        "id": "Byz-xOPD9Hut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare sets for CNN"
      ],
      "metadata": {
        "id": "H7-qckVzJ8bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# add channel dimension for CNN\n",
        "# -----------------------------\n",
        "X_train_cnn = X_train[..., np.newaxis]\n",
        "X_val_cnn   = X_val[..., np.newaxis]\n",
        "X_test_cnn  = X_test[..., np.newaxis]\n",
        "\n",
        "# print results\n",
        "print(\"Shapes after adding channel dimension:\")\n",
        "print(\"X_train:\", X_train_cnn.shape)\n",
        "print(\"X_val:  \", X_val_cnn.shape)\n",
        "print(\"X_test: \", X_test_cnn.shape)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# dataset is imbalanced, compute class weights for training set\n",
        "# -------------------------------------------------------------\n",
        "class_weights_values = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "\n",
        "# print results\n",
        "class_weights = {i: w for i, w in enumerate(class_weights_values)}\n",
        "print(\"\\nClass weights:\", class_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFd0k9N-9Jpi",
        "outputId": "9dc1f6f3-031b-4cbb-a5cb-85e40c689b50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes after adding channel dimension:\n",
            "X_train: (22027, 2500, 1)\n",
            "X_val:   (7003, 2500, 1)\n",
            "X_test:  (7297, 2500, 1)\n",
            "\n",
            "Class weights: {0: np.float64(0.8847606041131105), 1: np.float64(1.1497546716776281)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and evaluate different models"
      ],
      "metadata": {
        "id": "pUI82p7wKB1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### With computed class weights"
      ],
      "metadata": {
        "id": "_PTYnyF7P5XH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adjusted class weights\n",
        "class_weights = {0: 0.88, 1: 1.15}\n",
        "\n",
        "# ----------\n",
        "# define CNN\n",
        "# ----------\n",
        "model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=7, activation='relu', input_shape=(2500, 1)),\n",
        "    MaxPooling1D(pool_size=4),\n",
        "\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    MaxPooling1D(pool_size=4),\n",
        "\n",
        "    Conv1D(filters=256, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=4),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# -----------\n",
        "# train model\n",
        "# -----------\n",
        "history = model.fit(\n",
        "    X_train_cnn, y_train,\n",
        "    validation_data=(X_val_cnn, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=256,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --------\n",
        "# evaluate\n",
        "# --------\n",
        "print(\"\\n----------------------------------------\\n\\n-------------- EVALUATION --------------\\n\")\n",
        "print(f\"Used class weights: {class_weights}\\n\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test_cnn, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {test_acc*100:.2f}%\\nTest Loss: {test_loss:.4f}\\n\")\n",
        "\n",
        "# predictions\n",
        "y_pred_prob = model.predict(X_test_cnn, verbose=0)\n",
        "# convert probabilities to class labels (0 or 1)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# classification report (precision, recall, F1)\n",
        "report = classification_report(y_test, y_pred, target_names=['Normal', 'AFib'])\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kto_Ko6zPHIc",
        "outputId": "3c5955fb-48e6-4bec-c97a-6a5839db373c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 169ms/step - accuracy: 0.6128 - loss: 0.6560 - val_accuracy: 0.7985 - val_loss: 0.4776 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 105ms/step - accuracy: 0.7540 - loss: 0.5280 - val_accuracy: 0.5773 - val_loss: 0.5678 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 103ms/step - accuracy: 0.7811 - loss: 0.4903 - val_accuracy: 0.5232 - val_loss: 0.7718 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 97ms/step - accuracy: 0.7984 - loss: 0.4610 - val_accuracy: 0.5438 - val_loss: 0.7061 - learning_rate: 5.0000e-05\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "-------------- EVALUATION --------------\n",
            "\n",
            "Used class weights: {0: 0.88, 1: 1.15}\n",
            "\n",
            "Test Accuracy: 77.29%\n",
            "Test Loss: 0.5077\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3697 1588]\n",
            " [  69 1943]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.98      0.70      0.82      5285\n",
            "        AFib       0.55      0.97      0.70      2012\n",
            "\n",
            "    accuracy                           0.77      7297\n",
            "   macro avg       0.77      0.83      0.76      7297\n",
            "weighted avg       0.86      0.77      0.78      7297\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output is expected.\n",
        "\n",
        "EarlyStopping(...) rolls back to the weights used in the epoch with the lowest val_loss.\n",
        "\n",
        "Test accuracy is similar to the val_accuracy of that epoch. Recall on AFib is excellent and recall on N is low."
      ],
      "metadata": {
        "id": "DxeStO8xdLaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Increasing patience on early stopping"
      ],
      "metadata": {
        "id": "pxJV90l7QB7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adjusted class weights\n",
        "class_weights = {0: 0.88, 1: 1.15}\n",
        "\n",
        "# ----------\n",
        "# define CNN\n",
        "# ----------\n",
        "model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=7, activation='relu', input_shape=(2500, 1)),\n",
        "    MaxPooling1D(pool_size=4),\n",
        "\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    MaxPooling1D(pool_size=4),\n",
        "\n",
        "    Conv1D(filters=256, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=4),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# -----------\n",
        "# train model\n",
        "# -----------\n",
        "history = model.fit(\n",
        "    X_train_cnn, y_train,\n",
        "    validation_data=(X_val_cnn, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=256,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --------\n",
        "# evaluate\n",
        "# --------\n",
        "print(\"\\n----------------------------------------\\n\\n-------------- EVALUATION --------------\\n\")\n",
        "print(f\"Used class weights: {class_weights}\\n\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test_cnn, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {test_acc*100:.2f}%\\nTest Loss: {test_loss:.4f}\\n\")\n",
        "\n",
        "# predictions\n",
        "y_pred_prob = model.predict(X_test_cnn, verbose=0)\n",
        "# convert probabilities to class labels (0 or 1)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# classification report (precision, recall, F1)\n",
        "report = classification_report(y_test, y_pred, target_names=['Normal', 'AFib'])\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJKe7Xs7PbMo",
        "outputId": "a35f3932-2935-423f-a019-6f7dd907a600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 126ms/step - accuracy: 0.5704 - loss: 0.6715 - val_accuracy: 0.4391 - val_loss: 0.7466 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 92ms/step - accuracy: 0.7392 - loss: 0.5393 - val_accuracy: 0.6443 - val_loss: 0.5090 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 89ms/step - accuracy: 0.7680 - loss: 0.5114 - val_accuracy: 0.5483 - val_loss: 0.6893 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 90ms/step - accuracy: 0.7900 - loss: 0.4733 - val_accuracy: 0.5253 - val_loss: 0.8574 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 90ms/step - accuracy: 0.8013 - loss: 0.4520 - val_accuracy: 0.5114 - val_loss: 1.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 6/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 90ms/step - accuracy: 0.8028 - loss: 0.4470 - val_accuracy: 0.5145 - val_loss: 1.0573 - learning_rate: 5.0000e-05\n",
            "Epoch 7/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 91ms/step - accuracy: 0.8080 - loss: 0.4369 - val_accuracy: 0.5292 - val_loss: 1.0446 - learning_rate: 2.5000e-05\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "-------------- EVALUATION --------------\n",
            "\n",
            "Used class weights: {0: 0.88, 1: 1.15}\n",
            "\n",
            "Test Accuracy: 75.89%\n",
            "Test Loss: 0.4601\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3982 1303]\n",
            " [ 456 1556]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.90      0.75      0.82      5285\n",
            "        AFib       0.54      0.77      0.64      2012\n",
            "\n",
            "    accuracy                           0.76      7297\n",
            "   macro avg       0.72      0.76      0.73      7297\n",
            "weighted avg       0.80      0.76      0.77      7297\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing patience on early stopping makes the model more likely to predict N, thus reducing precision for both classes"
      ],
      "metadata": {
        "id": "ANd3irhKeSTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Trying different class weights"
      ],
      "metadata": {
        "id": "-3b8W0tFQdEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adjusted class weights\n",
        "class_weights = {0: 1.0, 1: 2.0}\n",
        "\n",
        "# ----------\n",
        "# define CNN\n",
        "# ----------\n",
        "model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=7, activation='relu', input_shape=(2500, 1)),\n",
        "    MaxPooling1D(pool_size=4),\n",
        "\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    MaxPooling1D(pool_size=4),\n",
        "\n",
        "    Conv1D(filters=256, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=4),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# -----------\n",
        "# train model\n",
        "# -----------\n",
        "history = model.fit(\n",
        "    X_train_cnn, y_train,\n",
        "    validation_data=(X_val_cnn, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=256,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --------\n",
        "# evaluate\n",
        "# --------\n",
        "print(\"\\n----------------------------------------\\n\\n-------------- EVALUATION --------------\\n\")\n",
        "print(f\"Used class weights: {class_weights}\\n\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test_cnn, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {test_acc*100:.2f}%\\nTest Loss: {test_loss:.4f}\\n\")\n",
        "\n",
        "# predictions\n",
        "y_pred_prob = model.predict(X_test_cnn, verbose=0)\n",
        "# convert probabilities to class labels (0 or 1)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# classification report (precision, recall, F1)\n",
        "report = classification_report(y_test, y_pred, target_names=['Normal', 'AFib'])\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg6GRH9DAP_X",
        "outputId": "32424b77-60fe-4cc9-cd73-5e9c17d1a217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 133ms/step - accuracy: 0.4991 - loss: 0.9442 - val_accuracy: 0.4768 - val_loss: 0.6926 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 97ms/step - accuracy: 0.6891 - loss: 0.7846 - val_accuracy: 0.4762 - val_loss: 0.7438 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 93ms/step - accuracy: 0.7399 - loss: 0.7107 - val_accuracy: 0.4611 - val_loss: 0.8889 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 94ms/step - accuracy: 0.7693 - loss: 0.6619 - val_accuracy: 0.4707 - val_loss: 0.9467 - learning_rate: 5.0000e-05\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "-------------- EVALUATION --------------\n",
            "\n",
            "Used class weights: {0: 1.0, 1: 2.0}\n",
            "\n",
            "Test Accuracy: 72.70%\n",
            "Test Loss: 0.5631\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3397 1888]\n",
            " [ 104 1908]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.97      0.64      0.77      5285\n",
            "        AFib       0.50      0.95      0.66      2012\n",
            "\n",
            "    accuracy                           0.73      7297\n",
            "   macro avg       0.74      0.80      0.72      7297\n",
            "weighted avg       0.84      0.73      0.74      7297\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adjusted class weights\n",
        "class_weights = {0: 1.0, 1: 1.5}\n",
        "\n",
        "# ----------\n",
        "# define CNN\n",
        "# ----------\n",
        "model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=7, activation='relu', input_shape=(2500, 1)),\n",
        "    MaxPooling1D(pool_size=4),\n",
        "\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    MaxPooling1D(pool_size=4),\n",
        "\n",
        "    Conv1D(filters=256, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=4),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# -----------\n",
        "# train model\n",
        "# -----------\n",
        "history = model.fit(\n",
        "    X_train_cnn, y_train,\n",
        "    validation_data=(X_val_cnn, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=256,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --------\n",
        "# evaluate\n",
        "# --------\n",
        "print(\"\\n----------------------------------------\\n\\n-------------- EVALUATION --------------\\n\")\n",
        "print(f\"Used class weights: {class_weights}\\n\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test_cnn, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {test_acc*100:.2f}%\\nTest Loss: {test_loss:.4f}\\n\")\n",
        "\n",
        "# predictions\n",
        "y_pred_prob = model.predict(X_test_cnn, verbose=0)\n",
        "# convert probabilities to class labels (0 or 1)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# classification report (precision, recall, F1)\n",
        "report = classification_report(y_test, y_pred, target_names=['Normal', 'AFib'])\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyX_4uxPOTgX",
        "outputId": "c0d54ca0-2294-4be7-ac1d-20c8d534c0ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 128ms/step - accuracy: 0.5502 - loss: 0.8138 - val_accuracy: 0.5173 - val_loss: 0.6725 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 92ms/step - accuracy: 0.7425 - loss: 0.6533 - val_accuracy: 0.5149 - val_loss: 0.6846 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 91ms/step - accuracy: 0.7712 - loss: 0.6083 - val_accuracy: 0.5203 - val_loss: 0.7631 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 92ms/step - accuracy: 0.7891 - loss: 0.5759 - val_accuracy: 0.5241 - val_loss: 0.7904 - learning_rate: 5.0000e-05\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "-------------- EVALUATION --------------\n",
            "\n",
            "Used class weights: {0: 1.0, 1: 1.5}\n",
            "\n",
            "Test Accuracy: 74.98%\n",
            "Test Loss: 0.5249\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3618 1667]\n",
            " [ 159 1853]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.96      0.68      0.80      5285\n",
            "        AFib       0.53      0.92      0.67      2012\n",
            "\n",
            "    accuracy                           0.75      7297\n",
            "   macro avg       0.74      0.80      0.73      7297\n",
            "weighted avg       0.84      0.75      0.76      7297\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adjusted class weights\n",
        "class_weights = {0: 1.0, 1: 1.3}\n",
        "\n",
        "# ----------\n",
        "# define CNN\n",
        "# ----------\n",
        "model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=7, activation='relu', input_shape=(2500, 1)),\n",
        "    MaxPooling1D(pool_size=4),\n",
        "\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    MaxPooling1D(pool_size=4),\n",
        "\n",
        "    Conv1D(filters=256, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=4),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# -----------\n",
        "# train model\n",
        "# -----------\n",
        "history = model.fit(\n",
        "    X_train_cnn, y_train,\n",
        "    validation_data=(X_val_cnn, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=256,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --------\n",
        "# evaluate\n",
        "# --------\n",
        "print(\"\\n----------------------------------------\\n\\n-------------- EVALUATION --------------\\n\")\n",
        "print(f\"Used class weights: {class_weights}\\n\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test_cnn, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {test_acc*100:.2f}%\\nTest Loss: {test_loss:.4f}\\n\")\n",
        "\n",
        "# predictions\n",
        "y_pred_prob = model.predict(X_test_cnn, verbose=0)\n",
        "# convert probabilities to class labels (0 or 1)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# classification report (precision, recall, F1)\n",
        "report = classification_report(y_test, y_pred, target_names=['Normal', 'AFib'])\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uYfnfl6OufV",
        "outputId": "c1c8d11e-a56b-46e9-e82a-2548aef47388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 129ms/step - accuracy: 0.5886 - loss: 0.7564 - val_accuracy: 0.1962 - val_loss: 0.9153 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 92ms/step - accuracy: 0.7274 - loss: 0.6227 - val_accuracy: 0.5411 - val_loss: 0.6113 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 92ms/step - accuracy: 0.7740 - loss: 0.5673 - val_accuracy: 0.5759 - val_loss: 0.5857 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 93ms/step - accuracy: 0.7932 - loss: 0.5365 - val_accuracy: 0.5192 - val_loss: 0.7896 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 96ms/step - accuracy: 0.7970 - loss: 0.5216 - val_accuracy: 0.5405 - val_loss: 0.7694 - learning_rate: 1.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 92ms/step - accuracy: 0.8096 - loss: 0.4925 - val_accuracy: 0.5266 - val_loss: 0.9544 - learning_rate: 5.0000e-05\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "-------------- EVALUATION --------------\n",
            "\n",
            "Used class weights: {0: 1.0, 1: 1.3}\n",
            "\n",
            "Test Accuracy: 73.48%\n",
            "Test Loss: 0.5755\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3586 1699]\n",
            " [ 236 1776]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.94      0.68      0.79      5285\n",
            "        AFib       0.51      0.88      0.65      2012\n",
            "\n",
            "    accuracy                           0.73      7297\n",
            "   macro avg       0.72      0.78      0.72      7297\n",
            "weighted avg       0.82      0.73      0.75      7297\n",
            "\n"
          ]
        }
      ]
    }
  ]
}